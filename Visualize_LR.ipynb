{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trantheanh/100DaysOfML/blob/master/Visualize_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxNnd_PJNZEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the required Keras modules containing model and layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "model1 = tf.keras.Sequential()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrTRK_oJN-IT",
        "colab_type": "code",
        "outputId": "ba74601a-e4d8-4b80-e319-98143d8f1a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "# # X axis\n",
        "# x = np.array(range(1,11))\n",
        "\n",
        "# # REAL FUNCTION Y axis\n",
        "# real_y = np.array([autoregressive_func([1,1])(t) for t in x])\n",
        "\n",
        "# # HYPOTHESIS FUNCTION Y axis\n",
        "# degree = 2#np.random.randint(low=1, high=10)\n",
        "# params = np.random.rand(degree) * 5\n",
        "# hypothesis_y = np.array([polynomio_func(params)(t) for t in x])\n",
        "\n",
        "# fig, ax = plt.subplots(2, 2, sharex=True)\n",
        "\n",
        "# fig.set_figheight(9)\n",
        "# fig.set_figwidth(12)\n",
        "\n",
        "# ax[0][0].plot(x, real_y, 'g')\n",
        "# # ax[0][0].axis([0, 10, 0, 200])\n",
        "# ax[0][0].set_ylabel(\"REAL FUNCTION\")\n",
        "\n",
        "# ax[0][1].plot(x, hypothesis_y, 'b')\n",
        "# # ax[0][1].axis([0, 10, 0, 200])\n",
        "# ax[0][1].set_ylabel(\"HYPOTHESIS FUNCTION\")\n",
        "\n",
        "# # ax[1][0].axis([0, 10, 0, 200])\n",
        "# ax[1][0].plot(x, real_y, 'g')\n",
        "# ax[1][0].plot(x, hypothesis_y, 'b')\n",
        "# ax[1][0].set_ylabel(\"2 FUNCTION\")\n",
        "\n",
        "# # ax[1][1].axis([0, 10, 0, 200])\n",
        "# ax[1][1].plot(x, real_y, x, hypothesis_y, color=\"black\")\n",
        "# ax[1][1].fill_between(x, real_y, hypothesis_y,  where=real_y>hypothesis_y, facecolor=\"red\", interpolate=True)\n",
        "# ax[1][1].fill_between(x, real_y, hypothesis_y,  where=real_y<hypothesis_y, facecolor=\"green\", interpolate=True)\n",
        "# ax[1][1].set_ylabel(\"THE DIFFERENCE\")\n",
        "# print(\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-90505a62c4ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized keyword arguments: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;31m# Case 1: distribution strategy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2932\u001b[0m     \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2934\u001b[0;31m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[1;32m   2935\u001b[0m                          \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
            "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOmFYuw5NOb5",
        "colab_type": "code",
        "outputId": "61500092-841b-4d3b-ac11-d2176a53a185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(tf.__version__)\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "# Making sure that the values are float so that we can get decimal points after division\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('Number of images in x_train', x_train.shape[0])\n",
        "print('Number of images in x_test', x_test.shape[0])\n",
        "\n",
        "# Creating a Sequential Model and adding the layers\n",
        "model = tf.keras.Sequential()\n",
        "model.add(layers.Conv2D(32, kernel_size=(3, 3), input_shape=input_shape))\n",
        "model.add(layers.Activation(activation=tf.nn.relu))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "model.add(layers.Conv2D(64, kernel_size=(3, 3)))\n",
        "model.add(layers.Activation(activation=tf.nn.relu))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "model.add(layers.Conv2D(128, kernel_size=(3, 3)))\n",
        "model.add(layers.Activation(activation=tf.nn.relu))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "model.add(layers.Flatten())  # Flattening the 2D arrays for fully connected layers\n",
        "model.add(layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(layers.Dropout(0.3))\n",
        "model.add(layers.Dense(10, activation=tf.nn.softmax))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# model.fit(x=x_train, y=y_train, epochs=10)\n",
        "\n",
        "model.fit(x=x_train, y=y_train, epochs=30, validation_split=0.2, batch_size=32)\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "GPU Available:  True\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "Number of images in x_train 60000\n",
            "Number of images in x_test 10000\n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/30\n",
            "48000/48000 [==============================] - 12s 254us/sample - loss: 0.1177 - acc: 0.9645 - val_loss: 0.0536 - val_acc: 0.9830\n",
            "Epoch 2/30\n",
            "48000/48000 [==============================] - 12s 248us/sample - loss: 0.0449 - acc: 0.9853 - val_loss: 0.0490 - val_acc: 0.9862\n",
            "Epoch 3/30\n",
            "48000/48000 [==============================] - 12s 248us/sample - loss: 0.0301 - acc: 0.9899 - val_loss: 0.0433 - val_acc: 0.9865\n",
            "Epoch 4/30\n",
            "48000/48000 [==============================] - 12s 247us/sample - loss: 0.0216 - acc: 0.9928 - val_loss: 0.0417 - val_acc: 0.9901\n",
            "Epoch 5/30\n",
            "48000/48000 [==============================] - 12s 246us/sample - loss: 0.0157 - acc: 0.9951 - val_loss: 0.0466 - val_acc: 0.9880\n",
            "Epoch 6/30\n",
            "48000/48000 [==============================] - 12s 251us/sample - loss: 0.0145 - acc: 0.9955 - val_loss: 0.0530 - val_acc: 0.9883\n",
            "Epoch 7/30\n",
            "48000/48000 [==============================] - 12s 247us/sample - loss: 0.0117 - acc: 0.9960 - val_loss: 0.0500 - val_acc: 0.9906\n",
            "Epoch 8/30\n",
            "48000/48000 [==============================] - 12s 246us/sample - loss: 0.0102 - acc: 0.9967 - val_loss: 0.0444 - val_acc: 0.9912\n",
            "Epoch 9/30\n",
            "48000/48000 [==============================] - 12s 249us/sample - loss: 0.0074 - acc: 0.9977 - val_loss: 0.0486 - val_acc: 0.9905\n",
            "Epoch 10/30\n",
            "48000/48000 [==============================] - 12s 246us/sample - loss: 0.0090 - acc: 0.9972 - val_loss: 0.0713 - val_acc: 0.9883\n",
            "Epoch 11/30\n",
            "48000/48000 [==============================] - 12s 246us/sample - loss: 0.0092 - acc: 0.9975 - val_loss: 0.0492 - val_acc: 0.9887\n",
            "Epoch 12/30\n",
            "48000/48000 [==============================] - 12s 246us/sample - loss: 0.0066 - acc: 0.9982 - val_loss: 0.0502 - val_acc: 0.9898\n",
            "Epoch 13/30\n",
            "48000/48000 [==============================] - 12s 247us/sample - loss: 0.0070 - acc: 0.9979 - val_loss: 0.0762 - val_acc: 0.9884\n",
            "Epoch 14/30\n",
            "48000/48000 [==============================] - 12s 246us/sample - loss: 0.0063 - acc: 0.9983 - val_loss: 0.0629 - val_acc: 0.9905\n",
            "Epoch 15/30\n",
            "48000/48000 [==============================] - 12s 247us/sample - loss: 0.0049 - acc: 0.9990 - val_loss: 0.0759 - val_acc: 0.9882\n",
            "Epoch 16/30\n",
            "48000/48000 [==============================] - 12s 248us/sample - loss: 0.0078 - acc: 0.9980 - val_loss: 0.0721 - val_acc: 0.9891\n",
            "Epoch 17/30\n",
            "48000/48000 [==============================] - 12s 247us/sample - loss: 0.0066 - acc: 0.9980 - val_loss: 0.0957 - val_acc: 0.9885\n",
            "Epoch 18/30\n",
            "48000/48000 [==============================] - 12s 248us/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0987 - val_acc: 0.9879\n",
            "Epoch 19/30\n",
            "48000/48000 [==============================] - 12s 248us/sample - loss: 0.0059 - acc: 0.9986 - val_loss: 0.0773 - val_acc: 0.9899\n",
            "Epoch 20/30\n",
            "48000/48000 [==============================] - 12s 246us/sample - loss: 0.0059 - acc: 0.9987 - val_loss: 0.1028 - val_acc: 0.9900\n",
            "Epoch 21/30\n",
            "48000/48000 [==============================] - 12s 247us/sample - loss: 0.0080 - acc: 0.9983 - val_loss: 0.0993 - val_acc: 0.9902\n",
            "Epoch 22/30\n",
            "48000/48000 [==============================] - 12s 246us/sample - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0865 - val_acc: 0.9912\n",
            "Epoch 23/30\n",
            "48000/48000 [==============================] - 12s 248us/sample - loss: 0.0041 - acc: 0.9990 - val_loss: 0.0950 - val_acc: 0.9900\n",
            "Epoch 24/30\n",
            "48000/48000 [==============================] - 12s 246us/sample - loss: 0.0040 - acc: 0.9988 - val_loss: 0.1078 - val_acc: 0.9891\n",
            "Epoch 25/30\n",
            "48000/48000 [==============================] - 12s 247us/sample - loss: 0.0086 - acc: 0.9983 - val_loss: 0.0991 - val_acc: 0.9900\n",
            "Epoch 26/30\n",
            "48000/48000 [==============================] - 12s 247us/sample - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1255 - val_acc: 0.9883\n",
            "Epoch 27/30\n",
            "48000/48000 [==============================] - 12s 246us/sample - loss: 0.0042 - acc: 0.9991 - val_loss: 0.1106 - val_acc: 0.9898\n",
            "Epoch 28/30\n",
            "48000/48000 [==============================] - 12s 247us/sample - loss: 0.0063 - acc: 0.9985 - val_loss: 0.1099 - val_acc: 0.9907\n",
            "Epoch 29/30\n",
            "48000/48000 [==============================] - 12s 249us/sample - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0990 - val_acc: 0.9912\n",
            "Epoch 30/30\n",
            "48000/48000 [==============================] - 12s 247us/sample - loss: 0.0041 - acc: 0.9991 - val_loss: 0.1197 - val_acc: 0.9904\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_fSIzgQLOE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class model():\n",
        "    \n",
        "    def __init__(self,\n",
        "                 X_train,\n",
        "                 Y_train,\n",
        "                 num_step = 10000,\n",
        "                 lr = 0.1):\n",
        "        self.X_train = X_train\n",
        "        self.Y_train = Y_train\n",
        "        self.num_step = num_step\n",
        "        self.lr = lr\n",
        "        \n",
        "    def initialize(self):\n",
        "        self.W = np.random.rand(1, self.X_train.shape[0])\n",
        "        self.b = 0\n",
        "        \n",
        "    # Prediction of datas point\n",
        "    # X : matrix (n x m)\n",
        "    # W : vector n dimension\n",
        "    # b : scalar\n",
        "    def predict(self,X):\n",
        "        Y_hat = np.dot(self.W, X) + self.b\n",
        "        return Y_hat\n",
        "\n",
        "    # Gradient on data set\n",
        "    # X : matrix (m x n)\n",
        "    # Y : matrix (m x 1)\n",
        "    # W : Vector n dimension\n",
        "    # b : scalar\n",
        "    def grad(self):\n",
        "        m = self.Y_train.shape[1] * 1.0\n",
        "        Y_hat = self.predict(self.X_train)\n",
        "        dW = 1/m * np.dot((Y_hat - self.Y_train), np.transpose(self.X_train))\n",
        "        db = 1/m * np.sum(Y_hat - self.Y_train)    \n",
        "        return {\"dW\":dW, \"db\" : db}\n",
        "\n",
        "    def loss(self):\n",
        "        m = self.Y_train.shape[1] * 1.0\n",
        "        Y_hat = self.predict(self.X_train) \n",
        "        return 1/(2.0*m) * np.sum(np.square(Y_hat - self.Y_train))\n",
        "\n",
        "    def train(self):\n",
        "        self.initialize()\n",
        "        self.losses = []\n",
        "        for i in range(self.num_step):\n",
        "            gradient = self.grad()\n",
        "            self.W = self.W - self.lr * gradient[\"dW\"]\n",
        "            self.b = self.b - self.lr * gradient[\"db\"]\n",
        "            self.losses.append(self.loss())\n",
        "\n",
        "            if i%500==0:\n",
        "                print(\"At step \" + str(i) + \" \" + str(self.losses[-1]))\n",
        "                \n",
        "    def train_single_step(self):\n",
        "        gradient = self.grad()\n",
        "        self.W = self.W - self.lr * gradient[\"dW\"]\n",
        "        self.b = self.b - self.lr * gradient[\"db\"]\n",
        "        print(self.loss())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chS8HzBuOHm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3771d909-e94b-4a04-836b-99a5b68e6a6d"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import animation\n",
        "\n",
        "# Define polynomio function\n",
        "def autoregressive_func(params=[0]):\n",
        "  \n",
        "  def func(x):\n",
        "    result = 0\n",
        "    if x < len(params):\n",
        "      return 1\n",
        "    \n",
        "    for i in range(len(params)):\n",
        "      result += params[i] * func(x-i-1)\n",
        "      \n",
        "    return result\n",
        "  \n",
        "  return func\n",
        "\n",
        "def polynomio_func(params=[0]):\n",
        "  \n",
        "  def func(x):\n",
        "    result = 0\n",
        "    for i in range(len(params)):\n",
        "      result += params[i] * (x**i)\n",
        "      \n",
        "    return result\n",
        "  \n",
        "  return func\n",
        "\n",
        "def real_function(x):\n",
        "  real_y = np.array([autoregressive_func([1,1])(t) for t in x])\n",
        "  return real_y\n",
        "\n",
        "def hypothesis_function(x, params):\n",
        "  hypothesis_y = np.array([polynomio_func(params)(t) for t in x])\n",
        "\n",
        "def draw_animation(x, real_function, hypothesis_function):\n",
        "  # First set up the figure, the axis, and the plot element we want to animate\n",
        "  fig = plt.figure()\n",
        "  ax = plt.axes(xlim=(0, 10), ylim=(0, 80))\n",
        "  \n",
        "  # Plot Real function with Green\n",
        "  real_y = real_function(x)\n",
        "  ax.plot(x, real_y, 'g')\n",
        "  \n",
        "  line, = ax.plot([], [], lw=2)\n",
        "\n",
        "  # initialization function: plot the background of each frame\n",
        "  def init():\n",
        "      line.set_data([], [])\n",
        "      return line,\n",
        "\n",
        "  # animation function.  This is called sequentially\n",
        "  def animate(i):\n",
        "      x = np.array(range(1,11))\n",
        "      \n",
        "      degree = np.random.randint(low=1, high=10)\n",
        "      params = np.random.rand(degree) * 5\n",
        "      hypothesis_y = hypothesis_function(x, params)\n",
        "      line.set_data(x, hypothesis_y)\n",
        "      return line,\n",
        "\n",
        "  # call the animator.  blit=True means only re-draw the parts that have changed.\n",
        "  anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                                 frames=200, interval=20, blit=True)\n",
        "\n",
        "  anim.save('basic_animation.mp4', fps=30, extra_args=['-vcodec', 'libx264'])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "draw_animation(\n",
        "    x=np.array(range(1,11)), \n",
        "    real_function=real_function, \n",
        "    hypothesis_function=hypothesis_function)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHFK5i4jEIZW7iEgc+n8rh9WaRaVcZAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI1zGiZEagHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}